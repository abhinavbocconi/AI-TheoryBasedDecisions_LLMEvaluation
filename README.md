# RnR LLM Evaluation Pipeline

A comprehensive evaluation system comparing human expert assessments with Large Language Model (LLM) evaluations of business theories for startup success prediction.

## ğŸ“Š Overview

This research pipeline evaluates **1,962 business theories** from **981 participants** across **2 time periods**, comparing human expert judgments with AI model assessments on multiple dimensions.

### Key Features
- **Dual LLM Evaluation**: Claude-4-Sonnet & GPT-o4-mini
- **10-Run Reliability**: Each theory evaluated 10 times per model for statistical robustness
- **Multi-Dimensional Assessment**: 5 quality dimensions + probability/confidence measures
- **Parallel Processing**: High-performance async evaluation (30+ theories simultaneously)
- **Statistical Analysis**: Cohen's kappa inter-rater reliability analysis included

## ğŸ—ï¸ System Architecture

```
Input Data (finalData_SS_981.csv)
    â†“
[ParallelAnalysis_performance.py] â†’ 5 Quality Dimensions Evaluation
[ParallelAnalysis_sps_conf.py]    â†’ SPS & Confidence Assessment
    â†“
Output CSVs with aggregated results (10 runs per model)
    â†“
[CohenK/ Analysis] â†’ Inter-rater reliability statistics
```

## ğŸ“‹ Requirements

### Python Dependencies
```bash
pip install anthropic openai pandas numpy asyncio aiohttp
```

### API Keys Required
- **Anthropic API Key** (Claude-4-Sonnet access)
- **OpenAI API Key** (GPT-o4-mini access)

## ğŸš€ Quick Start

### 1. Setup Environment
```bash
# Clone repository
git clone <repository-url>
cd RnRLLMEvaluation

# Create virtual environment
python3 -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Configure API Keys
```bash
# Set environment variables
export ANTHROPIC_API_KEY="your-anthropic-api-key-here"
export OPENAI_API_KEY="your-openai-api-key-here"

# Or create .env file (copy from .env.example)
echo "ANTHROPIC_API_KEY=your-anthropic-api-key-here" > .env
echo "OPENAI_API_KEY=your-openai-api-key-here" >> .env
```

### 3. Run Evaluations

#### Quality Dimensions Evaluation (5 metrics)
```bash
python3 ParallelAnalysis_performance.py
```
**Output**: `business_evaluation_PARALLEL_YYYYMMDD_HHMMSS.csv`

#### SPS & Confidence Evaluation
```bash
python3 ParallelAnalysis_sps_conf.py
```
**Output**: `sps_confidence_evaluation_PARALLEL_YYYYMMDD_HHMMSS.csv`

### 4. Statistical Analysis
```bash
cd CohenK/
python3 cohen_kappa_quality_integer_final.py
python3 cohen_kappa_quality_tercile_final.py
python3 cohen_kappa_sps_cit_final.py
```

## ğŸ“Š Evaluation Framework

### Performance Evaluation (5 Dimensions)
| Dimension | Scale | Description |
|-----------|-------|-------------|
| **Novelty** | 1-5 | How different from existing solutions? |
| **Feasibility & Scalability** | 1-5 | Likelihood to succeed and scale? |
| **Environmental Impact** | 1-5 | Planet-positive benefit level? |
| **Financial Impact** | 1-5 | Business value creation potential? |
| **Quality** | 1-5 | Overall solution quality? |

### SPS & Confidence Evaluation
| Metric | Scale | Description |
|--------|-------|-------------|
| **SPS** (Subjective Probability of Success) | 0-100% | Probability of 50% food waste reduction in 5 years |
| **CIT** (Confidence in Theory Assessment) | 1-7 | Confidence level in the assessment |

## ğŸ”§ Configuration

### Performance Settings
```python
MAX_CONCURRENT_THEORIES = 30      # Theories processed in parallel
ANTHROPIC_SEMAPHORE_SIZE = 50     # Max concurrent Anthropic API calls
OPENAI_SEMAPHORE_SIZE = 100       # Max concurrent OpenAI API calls
```

### Testing Mode
To test with limited theories (recommended for initial runs):
```python
# In either script, uncomment this line:
df_filtered = df_filtered.head(30)  # Test with 30 theories
```

## ğŸ“ File Structure

```
RnRLLMEvaluation/
â”œâ”€â”€ README.md                                    # This file
â”œâ”€â”€ requirements.txt                             # Python dependencies
â”œâ”€â”€ .env.example                                # Environment variables template
â”‚
â”œâ”€â”€ finalData_SS_981.csv                       # Input: 1,962 business theories
â”œâ”€â”€ system_prompt_performance.txt              # LLM prompt for quality evaluation
â”œâ”€â”€ system_prompt_sps_confidence.txt           # LLM prompt for SPS/confidence
â”‚
â”œâ”€â”€ ParallelAnalysis_performance.py             # Main: Quality dimensions evaluation
â”œâ”€â”€ ParallelAnalysis_sps_conf.py               # Main: SPS & confidence evaluation
â”‚
â”œâ”€â”€ business_evaluation_PARALLEL_*.csv         # Output: Quality evaluation results
â”œâ”€â”€ sps_confidence_evaluation_PARALLEL_*.csv   # Output: SPS/confidence results
â”‚
â”œâ”€â”€ CohenK/                                     # Statistical analysis folder
â”‚   â”œâ”€â”€ README.md                               # Cohen's kappa analysis documentation
â”‚   â”œâ”€â”€ business_evaluation_full.csv            # Combined dataset for analysis
â”‚   â”œâ”€â”€ cohen_kappa_quality_integer_final.py    # Conservative integer matching
â”‚   â”œâ”€â”€ cohen_kappa_quality_tercile_final.py    # Tercile categorization method
â”‚   â””â”€â”€ cohen_kappa_sps_cit_final.py           # SPS/CIT reliability analysis
â”‚
â””â”€â”€ ChatAnalysis/                               # Chat behavior analysis pipeline
    â”œâ”€â”€ README.md                               # Detailed pipeline documentation
    â”œâ”€â”€ cleanup.py                              # Step 1: Clean raw thread data
    â”œâ”€â”€ json_extract.py                         # Step 2: Extract messages from JSON
    â”œâ”€â”€ user_message_extract.py                 # Step 3: Extract user messages
    â”œâ”€â”€ merge_messages_fulldata.py              # Step 4-5: Merge with participant data
    â”œâ”€â”€ classify_messages.py                    # Step 6: LLM message classification
    â”œâ”€â”€ combine_classifications.py              # Step 7: 20-vote majority classification
    â”œâ”€â”€ analyze_chat_behavior.py                # Step 8: Query pattern analysis
    â”œâ”€â”€ subsample_condition_analysis.py         # Step 9: PhD/Experience Ã— Condition
    â”œâ”€â”€ single_query_analysis.py                # Step 10: Autopilot user analysis
    â”œâ”€â”€ generate_figures.py                     # Step 11: Publication figures
    â””â”€â”€ FullData/                               # Participant data merging
        â””â”€â”€ full_data_merge_threadID.py
```

## âš¡ Performance

### Typical Performance Metrics
- **Processing Speed**: ~20-25 seconds per theory
- **Parallel Efficiency**: ~1200x faster than sequential
- **API Calls per Theory**: 20 total (2 models Ã— 10 runs each)
- **Full Dataset Runtime**: ~11 hours for 1,962 theories

### Checkpoint System
- Automatic progress saving every completed theory
- Resume capability from interruptions
- Checkpoint files: `checkpoint_*_TIMESTAMP.pkl`

## ğŸ“ˆ Output Format

### Quality Evaluation Output
```csv
teresaID,theory,processing_timestamp,
anthropic_avg_novelty,anthropic_avg_feasibility_and_scalability,...,
openai_avg_novelty,openai_avg_feasibility_and_scalability,...,
cross_model_avg_novelty,cross_model_avg_feasibility_and_scalability,...
```

### SPS/Confidence Output
```csv
teresaID,theory,processing_timestamp,
anthropic_avg_sps_llm,anthropic_avg_cit_llm,anthropic_valid_runs,
openai_avg_sps_llm,openai_avg_cit_llm,openai_valid_runs,
cross_model_avg_sps_llm,cross_model_avg_cit_llm
```

## ğŸ”¬ Research Results Summary

### Key Findings from Cohen's Kappa Analysis
- **Human-LLM Agreement**: Generally modest (Îº < 0.40)
- **LLM-LLM Consistency**: Strong internal agreement (Îº > 0.50 on some dimensions)
- **Best Agreement**: Financial Impact and Quality dimensions (tercile method)
- **Confidence Calibration**: Major divergence between human vs. LLM confidence expression

### Statistical Significance
- **SPS Agreement**: Fair agreement (Îº â‰ˆ 0.25) between humans and LLMs
- **Quality Dimensions**: 30% show fair agreement using tercile categorization
- **Inter-LLM Reliability**: Substantial agreement on Environmental and Novelty assessments

## ğŸ’¬ Chat Analysis Pipeline

The `ChatAnalysis/` folder contains a complete pipeline for analyzing user chat behavior from the AI-assisted decision-making experiment.

### Overview
- **1,273 messages** from **603 participants** across **609 threads**
- Mean queries per user: **2.1** (range: 1â€“10)
- **39% single-query users** (autopilot behavior)

### Key Features
- **11-Step Pipeline**: From raw OpenAI thread exports to publication-ready figures
- **Dual LLM Classification**: GPT-5.1 + Claude Sonnet 4.5 with 20-vote majority
- **7 Intent Categories**: Task Delegation, Refinement Request, Evaluation Seeking, Information Seeking, Clarification, Acknowledgment, Other

### Key Findings

#### Autopilot â†’ Copilot Transition
| Round | Autopilot (Task Del.) | Copilot (Eval + Refine) |
|-------|----------------------|------------------------|
| 1st Query | 32% | 27% |
| 2nd Query | 23% | 33% |
| 3rd+ Query | 19% | 37% |

#### User-System-Problem Fit
| Subsample | Metric | General AI | Agentic AI | Diff |
|-----------|--------|------------|------------|------|
| **PhD** | Autopilot Rate | 22% | 34% | +12pp |
| **Experienced** | Copilot Rate | 26% | 36% | +9pp |

### Running the Pipeline
```bash
cd ChatAnalysis/
# See ChatAnalysis/README.md for detailed step-by-step instructions
python cleanup.py              # Step 1
python json_extract.py         # Step 2
python user_message_extract.py # Step 3
# ... continue through Step 11
```

ğŸ“– **See [ChatAnalysis/README.md](ChatAnalysis/README.md) for complete documentation.**

## ğŸš¨ Important Notes

### Security
- **Never commit API keys** to version control
- Use environment variables or `.env` files only
- Rotate API keys regularly

### Data Integrity
- **teresaID range**: 1-1962 (verified consistent across all files)
- **Missing data handling**: Automatic filtering of empty theory entries
- **Validation**: Built-in data range and format checks

### Cost Considerations
- **API Costs**: ~$50-100 for full dataset evaluation (varies by model pricing)
- **Rate Limits**: Built-in semaphore controls prevent API limit violations
- **Monitoring**: Progress tracking with estimated completion times

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch: `git checkout -b feature-name`
3. Ensure all tests pass with small dataset first
4. Submit pull request with clear description

## ğŸ“„ License

MIT License - See LICENSE file for details

## ğŸ“š Citation

```bibtex
@software{rnr_llm_evaluation,
  title={Human vs. LLM Business Theory Evaluation Pipeline},
  author={Abhinav Pandey},
  year={2024},
  url={https://github.com/abhinavbocconi/AI-TheoryBasedDecisions_LLMEvaluation}
}
```

## ğŸ’¡ Troubleshooting

### Common Issues
1. **API Key Errors**: Ensure environment variables are set correctly
2. **Rate Limiting**: Reduce semaphore sizes if hitting API limits
3. **Memory Issues**: Use smaller batch sizes for limited RAM systems
4. **Timeout Errors**: Check internet connection and API status

### Support
- Review error messages in console output
- Check checkpoint files for recovery points
- Validate input data format matches expected structure

---

**Last Updated**: September 2024 | **Python**: 3.7+ | **Status**: Production Ready